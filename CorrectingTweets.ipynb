{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install pytypo\n",
    "!pip install spellchecker\n",
    "!pip install pyspellchecker\n",
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "!pip install spacy-symspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "cols = ['notknow','text','target','id']\n",
    "cleaned_tweets = pd.read_csv(\"/kaggle/input/clean-sentiment140/clean_tweets.csv\",header=None, names=cols, encoding = 'ISO-8859-1', low_memory=False)\n",
    "# above line will be different depending on where you saved your data, and your file name\n",
    "cleaned_tweets.drop(columns=['notknow'], inplace=True)\n",
    "cleaned_tweets.drop(cleaned_tweets.index[0], inplace=True)\n",
    "cleaned_tweets.reset_index(inplace=True)\n",
    "cleaned_tweets.drop(columns=['index'], inplace=True)\n",
    "cleaned_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import pytypo\n",
    "spell = SpellChecker()\n",
    "spell.word_frequency.load_words(['microsoft', 'apple', 'google','twitter', 'facebook', 'amazon', 'texting', 'nah'])\n",
    "stop_words = set(stopwords.words('english')) \n",
    "contractions = ['ive', 'ya', 'b', 'ill', 'yall', 'im', 'youre', 'youll', 'theyre', 'didnt', 'cant', 'dont', 'howd', 'id', 'isnt', 'shouldnt', 'wheres', 'wont', 'wouldve', 'couldnt', 'hadnt', ' hasnt']\n",
    "for stop_w in contractions:\n",
    "    stop_words.add(stop_w)\n",
    "\n",
    "\n",
    "def correctSpelling(word_tokens):\n",
    "    \n",
    "    if(len(word_tokens) > 0):\n",
    "        misspelled = spell.unknown(word_tokens)\n",
    "        if(len(misspelled) > 0):\n",
    "            words_dict = dict()\n",
    "            for i in range(len(word_tokens)):\n",
    "                if word_tokens[i] in words_dict:\n",
    "                    words_dict[word_tokens[i].lower()].append(i)\n",
    "                else:\n",
    "                    words_dict[word_tokens[i].lower()] = [i]\n",
    "\n",
    "            for word in misspelled:\n",
    "                word_corrected = spell.correction(word)\n",
    "                for index in words_dict[word]:\n",
    "                    if(word_corrected == word):\n",
    "                        word_corrected = pytypo.correct(word)\n",
    "                    word_tokens[index] = word_corrected\n",
    "            # STOP WORDS\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return (\" \".join(filtered_sentence)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "fromtweet = 610000\n",
    "tok = WordPunctTokenizer()\n",
    "correct_tweet_texts = []\n",
    "correct_tweet_targets = []\n",
    "correct_tweet_ids = []\n",
    "print (\"Correcting spelling for tweets...\\n\")\n",
    "for i in tqdm(range(fromtweet,fromtweet+10000)):\n",
    "    if type(cleaned_tweets['text'][i]) is str:\n",
    "        clean_tuit = tok.tokenize(cleaned_tweets['text'][i])\n",
    "        #print(correctSpelling(clean_tuit))\n",
    "        correct_tweet_texts.append(correctSpelling(clean_tuit))\n",
    "        correct_tweet_targets.append(cleaned_tweets['target'][i])\n",
    "        correct_tweet_ids.append(cleaned_tweets['id'][i])\n",
    "\n",
    "cleaner_df = pd.DataFrame(correct_tweet_texts,columns=['text'])\n",
    "cleaner_df['target'] = correct_tweet_targets\n",
    "cleaner_df['id'] = correct_tweet_ids\n",
    "cleaner_df.to_csv('clean_corrected_tweets_'+ str(fromtweet) +'.csv',encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
